# Tokenization

# Natural Language Modeling
* Neural networks operate on vectors.
* How to represent language as a vector or a collection of vectors?
* Naive solution: map each letter in the alphabet to a vector. Each vector itself can be learned by the model.
+ can be used for any language that shares that alphabet
+ small alphabet
- creates large sequences of embedding vectors (or tokens)
* Other extreme: encoding each word
* Compromise: encode common 

## Side Effects
* LLMs are not great at counting words

# Biology

## Biological Sequences
## Other Data Structures
